import os
import sys
import time
import io
import subprocess
from subprocess import CalledProcessError
from typing import Dict, List, Tuple

import torch
import torchaudio
from torch.nn.utils.rnn import pad_sequence
from omegaconf import OmegaConf
from tqdm import tqdm

import warnings

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

from indextts.BigVGAN.models import BigVGAN as Generator
from indextts.gpt.model import UnifiedVoice
from indextts.utils.checkpoint import load_checkpoint
from indextts.utils.feature_extractors import MelSpectrogramFeatures

from indextts.utils.front import TextNormalizer, TextTokenizer


class IndexTTS:
    def __init__(
        self, cfg_path="checkpoints/config.yaml", model_dir="checkpoints", is_fp16=True, device=None, use_cuda_kernel=None,
    ):
        """
        Args:
            cfg_path (str): path to the config file.
            model_dir (str): path to the model directory.
            is_fp16 (bool): whether to use fp16.
            device (str): device to use (e.g., 'cuda:0', 'cpu'). If None, it will be set automatically based on the availability of CUDA or MPS.
            use_cuda_kernel (None | bool): whether to use BigVGan custom fused activation CUDA kernel, only for CUDA device.
        """
        if device is not None:
            self.device = device
            self.is_fp16 = False if device == "cpu" else is_fp16
            self.use_cuda_kernel = use_cuda_kernel is not None and use_cuda_kernel and device.startswith("cuda")
        elif torch.cuda.is_available():
            self.device = "cuda:0"
            self.is_fp16 = is_fp16
            self.use_cuda_kernel = use_cuda_kernel is None or use_cuda_kernel
        elif hasattr(torch, "mps") and torch.backends.mps.is_available():
            self.device = "mps"
            self.is_fp16 = False # Use float16 on MPS is overhead than float32
            self.use_cuda_kernel = False
        else:
            self.device = "cpu"
            self.is_fp16 = False
            self.use_cuda_kernel = False
            print(">> Be patient, it may take a while to run in CPU mode.")

        self.cfg = OmegaConf.load(cfg_path)
        self.model_dir = model_dir
        self.dtype = torch.float16 if self.is_fp16 else None
        self.stop_mel_token = self.cfg.gpt.stop_mel_token

        # Comment-off to load the VQ-VAE model for debugging tokenizer
        #   https://github.com/index-tts/index-tts/issues/34
        #
        # from indextts.vqvae.xtts_dvae import DiscreteVAE
        # self.dvae = DiscreteVAE(**self.cfg.vqvae)
        # self.dvae_path = os.path.join(self.model_dir, self.cfg.dvae_checkpoint)
        # load_checkpoint(self.dvae, self.dvae_path)
        # self.dvae = self.dvae.to(self.device)
        # if self.is_fp16:
        #     self.dvae.eval().half()
        # else:
        #     self.dvae.eval()
        # print(">> vqvae weights restored from:", self.dvae_path)
        self.gpt = UnifiedVoice(**self.cfg.gpt)
        self.gpt_path = os.path.join(self.model_dir, self.cfg.gpt_checkpoint)
        load_checkpoint(self.gpt, self.gpt_path)
        self.gpt = self.gpt.to(self.device)
        if self.is_fp16:
            self.gpt.eval().half()
        else:
            self.gpt.eval()
        print(">> GPT weights restored from:", self.gpt_path)
        if self.is_fp16:
            try:
                import deepspeed

                use_deepspeed = True
            except (ImportError, OSError, CalledProcessError) as e:
                use_deepspeed = False
                print(f">> DeepSpeedåŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ¨ç†: {e}")
                print("See more details https://www.deepspeed.ai/tutorials/advanced-install/")

            self.gpt.post_init_gpt2_config(use_deepspeed=use_deepspeed, kv_cache=True, half=True)
        else:
            self.gpt.post_init_gpt2_config(use_deepspeed=False, kv_cache=True, half=False)

        if self.use_cuda_kernel:
            # preload the CUDA kernel for BigVGAN
            try:
                from indextts.BigVGAN.alias_free_activation.cuda import load as anti_alias_activation_loader
                anti_alias_activation_cuda = anti_alias_activation_loader.load()
                print(">> Preload custom CUDA kernel for BigVGAN", anti_alias_activation_cuda)
            except Exception as e:
                print(">> Failed to load custom CUDA kernel for BigVGAN. Falling back to torch.", e, file=sys.stderr)
                print(" Reinstall with `pip install -e . --no-deps --no-build-isolation` to prebuild `anti_alias_activation_cuda` kernel.", file=sys.stderr)
                print(
                    "See more details: https://github.com/index-tts/index-tts/issues/164#issuecomment-2903453206", file=sys.stderr
                )
                self.use_cuda_kernel = False
        self.bigvgan = Generator(self.cfg.bigvgan, use_cuda_kernel=self.use_cuda_kernel)
        self.bigvgan_path = os.path.join(self.model_dir, self.cfg.bigvgan_checkpoint)
        vocoder_dict = torch.load(self.bigvgan_path, map_location="cpu")
        self.bigvgan.load_state_dict(vocoder_dict["generator"])
        self.bigvgan = self.bigvgan.to(self.device)
        # remove weight norm on eval mode
        self.bigvgan.remove_weight_norm()
        self.bigvgan.eval()
        print(">> bigvgan weights restored from:", self.bigvgan_path)
        self.bpe_path = os.path.join(self.model_dir, self.cfg.dataset["bpe_model"])
        self.normalizer = TextNormalizer()
        self.normalizer.load()
        print(">> TextNormalizer loaded")
        self.tokenizer = TextTokenizer(self.bpe_path, self.normalizer)
        print(">> bpe model loaded from:", self.bpe_path)
        # ç¼“å­˜å‚è€ƒéŸ³é¢‘melï¼š
        self.cache_audio_prompt = None
        self.cache_cond_mel = None
        # è¿›åº¦å¼•ç”¨æ˜¾ç¤ºï¼ˆå¯é€‰ï¼‰
        self.gr_progress = None
        self.model_version = self.cfg.version if hasattr(self.cfg, "version") else None

    def remove_long_silence(self, codes: torch.Tensor, silent_token=52, max_consecutive=30):
        """
        Shrink special tokens (silent_token and stop_mel_token) in codes
        codes: [B, T]
        """
        code_lens = []
        codes_list = []
        device = codes.device
        dtype = codes.dtype
        isfix = False
        for i in range(0, codes.shape[0]):
            code = codes[i]
            if not torch.any(code == self.stop_mel_token).item():
                len_ = code.size(0)
            else:
                stop_mel_idx = (code == self.stop_mel_token).nonzero(as_tuple=False)
                len_ = stop_mel_idx[0].item() if len(stop_mel_idx) > 0 else code.size(0)

            count = torch.sum(code == silent_token).item()
            if count > max_consecutive:
                # code = code.cpu().tolist()
                ncode_idx = []
                n = 0
                for k in range(len_):
                    assert code[k] != self.stop_mel_token, f"stop_mel_token {self.stop_mel_token} should be shrinked here"
                    if code[k] != silent_token:
                        ncode_idx.append(k)
                        n = 0
                    elif code[k] == silent_token and n < 10:
                        ncode_idx.append(k)
                        n += 1
                    # if (k == 0 and code[k] == 52) or (code[k] == 52 and code[k-1] == 52):
                    #    n += 1
                # new code
                len_ = len(ncode_idx)
                codes_list.append(code[ncode_idx])
                isfix = True
            else:
                # shrink to len_
                codes_list.append(code[:len_])
            code_lens.append(len_)
        if isfix:
            if len(codes_list) > 1:
                codes = pad_sequence(codes_list, batch_first=True, padding_value=self.stop_mel_token)
            else:
                codes = codes_list[0].unsqueeze(0)
        else:
            # unchanged
            pass
        # clip codes to max length
        max_len = max(code_lens)
        if max_len < codes.shape[1]:
            codes = codes[:, :max_len]
        code_lens = torch.tensor(code_lens, dtype=torch.long, device=device)
        return codes, code_lens

    def bucket_sentences(self, sentences, bucket_max_size=4) -> List[List[Dict]]:
        """
        Sentence data bucketing.
        if ``bucket_max_size=1``, return all sentences in one bucket.
        """
        outputs: List[Dict] = []
        for idx, sent in enumerate(sentences):
            outputs.append({"idx": idx, "sent": sent, "len": len(sent)})
       
        if len(outputs) > bucket_max_size:
            # split sentences into buckets by sentence length
            buckets: List[List[Dict]] = []
            factor = 1.5
            last_bucket = None
            last_bucket_sent_len_median = 0

            for sent in sorted(outputs, key=lambda x: x["len"]):
                current_sent_len = sent["len"]
                if current_sent_len == 0:
                    print(">> skip empty sentence")
                    continue
                if last_bucket is None \
                        or current_sent_len >= int(last_bucket_sent_len_median * factor) \
                        or len(last_bucket) >= bucket_max_size:
                    # new bucket
                    buckets.append([sent])
                    last_bucket = buckets[-1]
                    last_bucket_sent_len_median = current_sent_len
                else:
                    # current bucket can hold more sentences
                    last_bucket.append(sent) # sorted
                    mid = len(last_bucket) // 2
                    last_bucket_sent_len_median = last_bucket[mid]["len"]
            last_bucket=None
            # merge all buckets with size 1
            out_buckets: List[List[Dict]] = []
            only_ones: List[Dict] = []
            for b in buckets:
                if len(b) == 1:
                    only_ones.append(b[0])
                else:
                    out_buckets.append(b)
            if len(only_ones) > 0:
                # merge into previous buckets if possible
                # print("only_ones:", [(o["idx"], o["len"]) for o in only_ones])
                for i in range(len(out_buckets)):
                    b = out_buckets[i]
                    if len(b) < bucket_max_size:
                        b.append(only_ones.pop(0))
                        if len(only_ones) == 0:
                            break
                # combined all remaining sized 1 buckets
                if len(only_ones) > 0:
                    out_buckets.extend([only_ones[i:i+bucket_max_size] for i in range(0, len(only_ones), bucket_max_size)])
            return out_buckets
        return [outputs]

    def pad_tokens_cat(self, tokens_list: List[torch.Tensor]) -> torch.Tensor:
        if self.model_version and self.model_version >= 1.5:
            # 1.5ç‰ˆæœ¬ä»¥ä¸Šï¼Œç›´æ¥ä½¿ç”¨stop_text_token å³ä¾§å¡«å……ï¼Œå¡«å……åˆ°æœ€å¤§é•¿åº¦
            # [1, N] -> [N,]
            squeezed_tokens = [t.squeeze(0) for t in tokens_list]
            return pad_sequence(squeezed_tokens, batch_first=True, padding_value=self.cfg.gpt.stop_text_token, padding_side="right")
        max_len = max(t.size(1) for t in tokens_list)
        outputs = []
        for tensor in tokens_list:
            pad_len = max_len - tensor.size(1)
            if pad_len > 0:
                n = min(8, pad_len)
                tensor = torch.nn.functional.pad(tensor, (0, n), value=self.cfg.gpt.stop_text_token)
                tensor = torch.nn.functional.pad(tensor, (0, pad_len - n), value=self.cfg.gpt.start_text_token)
            tensor = tensor[:, :max_len]
            outputs.append(tensor)
        concatenated_tokens = torch.cat(outputs, dim=0)
        return concatenated_tokens

    def torch_empty_cache(self):
        try:
            if "cuda" in str(self.device):
                torch.cuda.empty_cache()
            elif "mps" in str(self.device):
                torch.mps.empty_cache()
        except Exception as e:
            pass

    def _set_gr_progress(self, value, desc):
        if self.gr_progress is not None:
            self.gr_progress(value, desc=desc)

    # å¿«é€Ÿæ¨ç†ï¼šå¯¹äº"å¤šå¥é•¿æ–‡æœ¬"ï¼Œå¯å®ç°è‡³å°‘ 2~10 å€ä»¥ä¸Šçš„é€Ÿåº¦æå‡~ ï¼ˆFirst modified by sunnyboxs 2025-04-16ï¼‰
    def infer_fast(self, audio_prompt, text, output_path, verbose=False, max_text_tokens_per_sentence=100, sentences_bucket_max_size=4, **generation_kwargs):
        """
        Args:
            ``max_text_tokens_per_sentence``: åˆ†å¥çš„æœ€å¤§tokenæ•°ï¼Œé»˜è®¤``100``ï¼Œå¯ä»¥æ ¹æ®GPUç¡¬ä»¶æƒ…å†µè°ƒæ•´
                - è¶Šå°ï¼Œbatch è¶Šå¤šï¼Œæ¨ç†é€Ÿåº¦è¶Š*å¿«*ï¼Œå ç”¨å†…å­˜æ›´å¤šï¼Œå¯èƒ½å½±å“è´¨é‡
                - è¶Šå¤§ï¼Œbatch è¶Šå°‘ï¼Œæ¨ç†é€Ÿåº¦è¶Š*æ…¢*ï¼Œå ç”¨å†…å­˜å’Œè´¨é‡æ›´æ¥è¿‘äºéå¿«é€Ÿæ¨ç†
            ``sentences_bucket_max_size``: åˆ†å¥åˆ†æ¡¶çš„æœ€å¤§å®¹é‡ï¼Œé»˜è®¤``4``ï¼Œå¯ä»¥æ ¹æ®GPUå†…å­˜è°ƒæ•´
                - è¶Šå¤§ï¼Œbucketæ•°é‡è¶Šå°‘ï¼Œbatchè¶Šå¤šï¼Œæ¨ç†é€Ÿåº¦è¶Š*å¿«*ï¼Œå ç”¨å†…å­˜æ›´å¤šï¼Œå¯èƒ½å½±å“è´¨é‡
                - è¶Šå°ï¼Œbucketæ•°é‡è¶Šå¤šï¼Œbatchè¶Šå°‘ï¼Œæ¨ç†é€Ÿåº¦è¶Š*æ…¢*ï¼Œå ç”¨å†…å­˜å’Œè´¨é‡æ›´æ¥è¿‘äºéå¿«é€Ÿæ¨ç†
        """
        print(">> start fast inference...")
        
        self._set_gr_progress(0, "start fast inference...")
        if verbose:
            print(f"origin text:{text}")
        start_time = time.perf_counter()

        # å¦‚æœå‚è€ƒéŸ³é¢‘æ”¹å˜äº†ï¼Œæ‰éœ€è¦é‡æ–°ç”Ÿæˆ cond_mel, æå‡é€Ÿåº¦
        if self.cache_cond_mel is None or self.cache_audio_prompt != audio_prompt:
            audio, sr = torchaudio.load(audio_prompt)
            audio = torch.mean(audio, dim=0, keepdim=True)
            if audio.shape[0] > 1:
                audio = audio[0].unsqueeze(0)
            audio = torchaudio.transforms.Resample(sr, 24000)(audio)
            cond_mel = MelSpectrogramFeatures()(audio).to(self.device)
            cond_mel_frame = cond_mel.shape[-1]
            if verbose:
                print(f"cond_mel shape: {cond_mel.shape}", "dtype:", cond_mel.dtype)

            self.cache_audio_prompt = audio_prompt
            self.cache_cond_mel = cond_mel
        else:
            cond_mel = self.cache_cond_mel
            cond_mel_frame = cond_mel.shape[-1]
            pass

        auto_conditioning = cond_mel
        cond_mel_lengths = torch.tensor([cond_mel_frame], device=self.device)

        # text_tokens
        text_tokens_list = self.tokenizer.tokenize(text)

        sentences = self.tokenizer.split_sentences(text_tokens_list, max_tokens_per_sentence=max_text_tokens_per_sentence)
        if verbose:
            print(">> text token count:", len(text_tokens_list))
            print("   splited sentences count:", len(sentences))
            print("   max_text_tokens_per_sentence:", max_text_tokens_per_sentence)
            print(*sentences, sep="\n")
        do_sample = generation_kwargs.pop("do_sample", True)
        top_p = generation_kwargs.pop("top_p", 0.8)
        top_k = generation_kwargs.pop("top_k", 30)
        temperature = generation_kwargs.pop("temperature", 1.0)
        autoregressive_batch_size = 1
        length_penalty = generation_kwargs.pop("length_penalty", 0.0)
        num_beams = generation_kwargs.pop("num_beams", 3)
        repetition_penalty = generation_kwargs.pop("repetition_penalty", 10.0)
        max_mel_tokens = generation_kwargs.pop("max_mel_tokens", 600)
        sampling_rate = 24000
        # lang = "EN"
        # lang = "ZH"
        wavs = []
        gpt_gen_time = 0
        gpt_forward_time = 0
        bigvgan_time = 0

        # text processing
        all_text_tokens: List[List[torch.Tensor]] = []
        self._set_gr_progress(0.1, "text processing...")
        bucket_max_size = sentences_bucket_max_size if self.device != "cpu" else 1
        all_sentences = self.bucket_sentences(sentences, bucket_max_size=bucket_max_size)
        bucket_count = len(all_sentences)
        if verbose:
            print(">> sentences bucket_count:", bucket_count,
                  "bucket sizes:", [(len(s), [t["idx"] for t in s]) for s in all_sentences],
                  "bucket_max_size:", bucket_max_size)
        for sentences in all_sentences:
            temp_tokens: List[torch.Tensor] = []
            all_text_tokens.append(temp_tokens)
            for item in sentences:
                sent = item["sent"]
                text_tokens = self.tokenizer.convert_tokens_to_ids(sent)
                text_tokens = torch.tensor(text_tokens, dtype=torch.int32, device=self.device).unsqueeze(0)
                if verbose:
                    print(text_tokens)
                    print(f"text_tokens shape: {text_tokens.shape}, text_tokens type: {text_tokens.dtype}")
                    # debug tokenizer
                    text_token_syms = self.tokenizer.convert_ids_to_tokens(text_tokens[0].tolist())
                    print("text_token_syms is same as sentence tokens", text_token_syms == sent) 
                temp_tokens.append(text_tokens)
        
            
        # Sequential processing of bucketing data
        all_batch_num = sum(len(s) for s in all_sentences)
        all_batch_codes = []
        processed_num = 0
        for item_tokens in all_text_tokens:
            batch_num = len(item_tokens)
            if batch_num > 1:
                batch_text_tokens = self.pad_tokens_cat(item_tokens)
            else:
                batch_text_tokens = item_tokens[0]
            processed_num += batch_num
            # gpt speech
            self._set_gr_progress(0.2 + 0.3 * processed_num/all_batch_num, f"gpt inference speech... {processed_num}/{all_batch_num}")
            m_start_time = time.perf_counter()
            with torch.no_grad():
                with torch.autocast(batch_text_tokens.device.type, enabled=self.dtype is not None, dtype=self.dtype):
                    temp_codes = self.gpt.inference_speech(auto_conditioning, batch_text_tokens,
                                        cond_mel_lengths=cond_mel_lengths,
                                        # text_lengths=text_len,
                                        do_sample=do_sample,
                                        top_p=top_p,
                                        top_k=top_k,
                                        temperature=temperature,
                                        num_return_sequences=autoregressive_batch_size,
                                        length_penalty=length_penalty,
                                        num_beams=num_beams,
                                        repetition_penalty=repetition_penalty,
                                        max_generate_length=max_mel_tokens,
                                        **generation_kwargs)
                    all_batch_codes.append(temp_codes)
            gpt_gen_time += time.perf_counter() - m_start_time

        # gpt latent
        self._set_gr_progress(0.5, "gpt inference latents...")
        all_idxs = []
        all_latents = []
        has_warned = False
        for batch_codes, batch_tokens, batch_sentences in zip(all_batch_codes, all_text_tokens, all_sentences):
            for i in range(batch_codes.shape[0]):
                codes = batch_codes[i]  # [x]
                if not has_warned and codes[-1] != self.stop_mel_token:
                    warnings.warn(
                        f"WARN: generation stopped due to exceeding `max_mel_tokens` ({max_mel_tokens}). "
                        f"Consider reducing `max_text_tokens_per_sentence`({max_text_tokens_per_sentence}) or increasing `max_mel_tokens`.",
                        category=RuntimeWarning
                    )
                    has_warned = True
                codes = codes.unsqueeze(0)  # [x] -> [1, x]
                if verbose:
                    print("codes:", codes.shape)
                    print(codes)
                codes, code_lens = self.remove_long_silence(codes, silent_token=52, max_consecutive=30)
                if verbose:
                    print("fix codes:", codes.shape)
                    print(codes)
                    print("code_lens:", code_lens)
                text_tokens = batch_tokens[i]
                all_idxs.append(batch_sentences[i]["idx"])
                m_start_time = time.perf_counter()
                with torch.no_grad():
                    with torch.autocast(text_tokens.device.type, enabled=self.dtype is not None, dtype=self.dtype):
                        latent = \
                            self.gpt(auto_conditioning, text_tokens,
                                        torch.tensor([text_tokens.shape[-1]], device=text_tokens.device), codes,
                                        code_lens*self.gpt.mel_length_compression,
                                        cond_mel_lengths=torch.tensor([auto_conditioning.shape[-1]], device=text_tokens.device),
                                        return_latent=True, clip_inputs=False)
                        gpt_forward_time += time.perf_counter() - m_start_time
                        all_latents.append(latent)
        del all_batch_codes, all_text_tokens, all_sentences
        # bigvgan chunk
        chunk_size = 2
        all_latents = [all_latents[all_idxs.index(i)] for i in range(len(all_latents))]
        if verbose:
            print(">> all_latents:", len(all_latents))
            print("  latents length:", [l.shape[1] for l in all_latents])
        chunk_latents = [all_latents[i : i + chunk_size] for i in range(0, len(all_latents), chunk_size)]
        chunk_length = len(chunk_latents)
        latent_length = len(all_latents)

        # bigvgan chunk decode
        self._set_gr_progress(0.7, "bigvgan decode...")
        tqdm_progress = tqdm(total=latent_length, desc="bigvgan")
        for items in chunk_latents:
            tqdm_progress.update(len(items))
            latent = torch.cat(items, dim=1)
            with torch.no_grad():
                with torch.autocast(latent.device.type, enabled=self.dtype is not None, dtype=self.dtype):
                    m_start_time = time.perf_counter()
                    wav, _ = self.bigvgan(latent, auto_conditioning.transpose(1, 2))
                    bigvgan_time += time.perf_counter() - m_start_time
                    wav = wav.squeeze(1)
                    pass
            wav = torch.clamp(32767 * wav, -32767.0, 32767.0)
            wavs.append(wav.cpu()) # to cpu before saving

        # clear cache
        tqdm_progress.close()  # ç¡®ä¿è¿›åº¦æ¡è¢«å…³é—­
        del all_latents, chunk_latents
        end_time = time.perf_counter()
        self.torch_empty_cache()

        # wav audio output
        self._set_gr_progress(0.9, "save audio...")
        wav = torch.cat(wavs, dim=1)
        wav_length = wav.shape[-1] / sampling_rate
        print(f">> Reference audio length: {cond_mel_frame * 256 / sampling_rate:.2f} seconds")
        print(f">> gpt_gen_time: {gpt_gen_time:.2f} seconds")
        print(f">> gpt_forward_time: {gpt_forward_time:.2f} seconds")
        print(f">> bigvgan_time: {bigvgan_time:.2f} seconds")
        print(f">> Total fast inference time: {end_time - start_time:.2f} seconds")
        print(f">> Generated audio length: {wav_length:.2f} seconds")
        print(f">> [fast] bigvgan chunk_length: {chunk_length}")
        print(f">> [fast] batch_num: {all_batch_num} bucket_max_size: {bucket_max_size}", f"bucket_count: {bucket_count}" if bucket_max_size > 1 else "")
        print(f">> [fast] RTF: {(end_time - start_time) / wav_length:.4f}")

        # save audio
        wav = wav.cpu()  # to cpu
        if output_path:
            # ç›´æ¥ä¿å­˜éŸ³é¢‘åˆ°æŒ‡å®šè·¯å¾„ä¸­
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            torchaudio.save(output_path, wav.type(torch.int16), sampling_rate)
            print(">> wav file saved to:", output_path)
            return output_path
        else:
            # è¿”å›ä»¥ç¬¦åˆGradioçš„æ ¼å¼è¦æ±‚
            wav_data = wav.type(torch.int16)
            wav_data = wav_data.numpy().T
            return (sampling_rate, wav_data)

    # æµå¼æ¨ç†æ¨¡å¼ - é€å¥ç”ŸæˆéŸ³é¢‘ç‰‡æ®µ
    def infer_stream(self, audio_prompt, text, verbose=False, max_text_tokens_per_sentence=120, **generation_kwargs):
        """
        æµå¼æ¨ç†å‡½æ•°ï¼Œé€å¥ç”ŸæˆéŸ³é¢‘ç‰‡æ®µ
        
        Args:
            audio_prompt: å‚è€ƒéŸ³é¢‘è·¯å¾„
            text: è¦åˆæˆçš„æ–‡æœ¬
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            max_text_tokens_per_sentence: æ¯å¥æœ€å¤§tokenæ•°
            **generation_kwargs: ç”Ÿæˆå‚æ•°
            
        Yields:
            dict: åŒ…å«éŸ³é¢‘ç‰‡æ®µä¿¡æ¯çš„å­—å…¸
                - audio_chunk: torch.Tensor, éŸ³é¢‘æ•°æ® (float32, ç¡®ä¿å…¼å®¹æ€§)
                - sample_rate: int, é‡‡æ ·ç‡
                - sentence_index: int, å½“å‰å¥å­ç´¢å¼• (ä»0å¼€å§‹)
                - total_sentences: int, æ€»å¥å­æ•°
                - sentence_text: str, å½“å‰å¥å­çš„tokenæ–‡æœ¬
                - timing_info: dict, æ—¶é—´ç»Ÿè®¡ä¿¡æ¯
        """
        print(">> start streaming inference...")
        print(f"ğŸš€ [DEBUG] infer_stream å¼€å§‹å¤„ç†ï¼Œè®¾å¤‡: {self.device}")
        if verbose:
            print(f"origin text:{text}")
        start_time = time.perf_counter()
        
        # æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        if "cuda" in str(self.device):
            try:
                import torch
                gpu_memory_before = torch.cuda.memory_allocated() / 1024**3  # GB
                print(f"ğŸ” [DEBUG] GPUå†…å­˜ä½¿ç”¨ (å¼€å§‹): {gpu_memory_before:.2f} GB")
            except:
                pass

        # å¦‚æœå‚è€ƒéŸ³é¢‘æ”¹å˜äº†ï¼Œæ‰éœ€è¦é‡æ–°ç”Ÿæˆ cond_mel, æå‡é€Ÿåº¦
        cond_mel_start = time.perf_counter()
        if self.cache_cond_mel is None or self.cache_audio_prompt != audio_prompt:
            print(f"ğŸµ [DEBUG] å¼€å§‹å¤„ç†å‚è€ƒéŸ³é¢‘: {audio_prompt}")
            audio, sr = torchaudio.load(audio_prompt)
            audio = torch.mean(audio, dim=0, keepdim=True)
            if audio.shape[0] > 1:
                audio = audio[0].unsqueeze(0)
            audio = torchaudio.transforms.Resample(sr, 24000)(audio)
            cond_mel = MelSpectrogramFeatures()(audio).to(self.device)
            cond_mel_frame = cond_mel.shape[-1]
            if verbose:
                print(f"cond_mel shape: {cond_mel.shape}", "dtype:", cond_mel.dtype)

            self.cache_audio_prompt = audio_prompt
            self.cache_cond_mel = cond_mel
            print(f"âœ… [DEBUG] å‚è€ƒéŸ³é¢‘å¤„ç†å®Œæˆï¼Œè€—æ—¶: {time.perf_counter() - cond_mel_start:.3f}s")
        else:
            cond_mel = self.cache_cond_mel
            cond_mel_frame = cond_mel.shape[-1]
            print(f"ğŸ”„ [DEBUG] ä½¿ç”¨ç¼“å­˜çš„å‚è€ƒéŸ³é¢‘ï¼Œè€—æ—¶: {time.perf_counter() - cond_mel_start:.3f}s")

        # æ–‡æœ¬å¤„ç†
        text_process_start = time.perf_counter()
        auto_conditioning = cond_mel
        text_tokens_list = self.tokenizer.tokenize(text)
        sentences = self.tokenizer.split_sentences(text_tokens_list, max_text_tokens_per_sentence)
        text_process_time = time.perf_counter() - text_process_start
        
        print(f"ğŸ“ [DEBUG] æ–‡æœ¬å¤„ç†å®Œæˆï¼Œè€—æ—¶: {text_process_time:.3f}s")
        print(f"ğŸ“Š [DEBUG] æ–‡æœ¬ç»Ÿè®¡: æ€»token={len(text_tokens_list)}, å¥å­æ•°={len(sentences)}, æ¯å¥æœ€å¤§token={max_text_tokens_per_sentence}")
        
        if verbose:
            print("text token count:", len(text_tokens_list))
            print("sentences count:", len(sentences))
            print("max_text_tokens_per_sentence:", max_text_tokens_per_sentence)
            print(*sentences, sep="\n")
        
        # æå–ç”Ÿæˆå‚æ•°
        params_start = time.perf_counter()
        do_sample = generation_kwargs.pop("do_sample", True)
        top_p = generation_kwargs.pop("top_p", 0.8)
        top_k = generation_kwargs.pop("top_k", 30)
        temperature = generation_kwargs.pop("temperature", 1.0)
        autoregressive_batch_size = 1
        length_penalty = generation_kwargs.pop("length_penalty", 0.0)
        num_beams = generation_kwargs.pop("num_beams", 3)
        repetition_penalty = generation_kwargs.pop("repetition_penalty", 10.0)
        max_mel_tokens = generation_kwargs.pop("max_mel_tokens", 600)
        sampling_rate = 24000
        params_time = time.perf_counter() - params_start
        print(f"âš™ï¸ [DEBUG] å‚æ•°å¤„ç†å®Œæˆï¼Œè€—æ—¶: {params_time:.3f}s")
        
        total_sentences = len(sentences)
        has_warned = False
        cumulative_gpt_gen_time = 0
        cumulative_gpt_forward_time = 0
        cumulative_bigvgan_time = 0
        
        print(f"ğŸ”„ [DEBUG] å¼€å§‹é€å¥å¤„ç† {total_sentences} ä¸ªå¥å­")
        
        for sentence_idx, sent in enumerate(sentences):
            sentence_start_time = time.perf_counter()
            print(f"\nğŸ”„ [DEBUG] === å¤„ç†ç¬¬ {sentence_idx + 1}/{total_sentences} å¥ ===")
            
            # æ£€æŸ¥GPUå†…å­˜
            if "cuda" in str(self.device):
                try:
                    gpu_memory_current = torch.cuda.memory_allocated() / 1024**3  # GB
                    print(f"ğŸ” [DEBUG] å½“å‰GPUå†…å­˜: {gpu_memory_current:.2f} GB")
                except:
                    pass
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ¸…ç†GPTæ¨¡å‹çš„KVç¼“å­˜ï¼Œé˜²æ­¢è·¨å¥å­ç´¯ç§¯
            if hasattr(self.gpt, 'inference_model') and hasattr(self.gpt.inference_model, 'cached_mel_emb'):
                # é‡æ–°è®¾ç½®mel embeddingç¼“å­˜ï¼Œæ¸…ç†ä¹‹å‰çš„çŠ¶æ€
                self.gpt.inference_model.store_mel_emb(auto_conditioning)
                print(f"ğŸ§  [DEBUG] æ¸…ç†GPT KVç¼“å­˜å¹¶é‡è®¾mel embedding")
            
            # æ–‡æœ¬tokenå¤„ç†
            token_start = time.perf_counter()
            text_tokens = self.tokenizer.convert_tokens_to_ids(sent)
            text_tokens = torch.tensor(text_tokens, dtype=torch.int32, device=self.device).unsqueeze(0)
            token_time = time.perf_counter() - token_start
            print(f"ğŸ“ [DEBUG] Tokenå¤„ç†è€—æ—¶: {token_time:.3f}s, shape: {text_tokens.shape}")
            
            if verbose:
                print(f"\n--- Processing sentence {sentence_idx + 1}/{total_sentences} ---")
                print(f"text_tokens shape: {text_tokens.shape}, text_tokens type: {text_tokens.dtype}")

            # GPT inference_speech ç”Ÿæˆcodes
            print(f"ğŸ§  [DEBUG] å¼€å§‹GPT inference_speech...")
            gpt_gen_start = time.perf_counter()
            with torch.no_grad():
                with torch.autocast(text_tokens.device.type, enabled=self.dtype is not None, dtype=self.dtype):
                    codes = self.gpt.inference_speech(auto_conditioning, text_tokens,
                                                        cond_mel_lengths=torch.tensor([auto_conditioning.shape[-1]],
                                                                                      device=text_tokens.device),
                                                        do_sample=do_sample,
                                                        top_p=top_p,
                                                        top_k=top_k,
                                                        temperature=temperature,
                                                        num_return_sequences=autoregressive_batch_size,
                                                        length_penalty=length_penalty,
                                                        num_beams=num_beams,
                                                        repetition_penalty=repetition_penalty,
                                                        max_generate_length=max_mel_tokens,
                                                        **generation_kwargs)
            gpt_gen_time = time.perf_counter() - gpt_gen_start
            cumulative_gpt_gen_time += gpt_gen_time
            print(f"âœ… [DEBUG] GPT inference_speechå®Œæˆï¼Œè€—æ—¶: {gpt_gen_time:.3f}s")
            
            # ğŸ” æ£€æŸ¥GPTç”Ÿæˆåçš„å†…å­˜
            if "cuda" in str(self.device):
                try:
                    gpu_memory_after_gpt_gen = torch.cuda.memory_allocated() / 1024**3  # GB
                    print(f"ğŸ” [DEBUG] GPTç”ŸæˆåGPUå†…å­˜: {gpu_memory_after_gpt_gen:.2f} GB")
                except:
                    pass
            
            if not has_warned and (codes[:, -1] != self.stop_mel_token).any():
                warnings.warn(
                    f"WARN: generation stopped due to exceeding `max_mel_tokens` ({max_mel_tokens}). "
                    f"Input text tokens: {text_tokens.shape[1]}. "
                    f"Consider reducing `max_text_tokens_per_sentence`({max_text_tokens_per_sentence}) or increasing `max_mel_tokens`.",
                    category=RuntimeWarning
                )
                has_warned = True

            # ğŸ¯ ä¿®å¤codeså¤„ç†é€»è¾‘ - æ›´å®‰å…¨çš„ç±»å‹æ£€æŸ¥å’Œå¤„ç†
            codes_process_start = time.perf_counter()
            
            # ç»Ÿä¸€å¤„ç†codes - ç¡®ä¿å§‹ç»ˆæ˜¯Tensoræ ¼å¼
            # ä½¿ç”¨æ›´å®‰å…¨çš„ç±»å‹æ£€æŸ¥æ–¹å¼ï¼Œé¿å…linteré”™è¯¯
            if hasattr(codes, 'sequences') and not isinstance(codes, torch.Tensor):
                # GenerateOutputå¯¹è±¡ï¼Œæå–sequences
                codes = getattr(codes, 'sequences')
                print(f"ğŸ”§ [DEBUG] ä»GenerateOutputæå–sequences")
            elif isinstance(codes, torch.Tensor):
                # å·²ç»æ˜¯Tensor
                print(f"ğŸ”§ [DEBUG] codeså·²ç»æ˜¯Tensoræ ¼å¼")
            else:
                # å…¶ä»–æƒ…å†µï¼Œå°è¯•è½¬æ¢
                codes = torch.tensor(codes, device=self.device)
                print(f"ğŸ”§ [DEBUG] è½¬æ¢codesä¸ºTensor")
            
            # ç¡®ä¿codesæ˜¯Tensorå¹¶ä¸”ç»´åº¦æ­£ç¡®
            if isinstance(codes, torch.Tensor) and codes.dim() == 1:
                codes = codes.unsqueeze(0)
            
            code_lens = torch.tensor([codes.shape[-1]], device=codes.device, dtype=torch.long)
            codes_process_time = time.perf_counter() - codes_process_start
            print(f"ğŸ”§ [DEBUG] Codeså¤„ç†è€—æ—¶: {codes_process_time:.3f}s, shape: {codes.shape}")
            
            if verbose:
                print(f"codes shape: {codes.shape}, codes type: {codes.dtype}")
                print(f"code len: {code_lens}")

            # ç§»é™¤è¶…é•¿é™éŸ³
            silence_start = time.perf_counter()
            codes, code_lens = self.remove_long_silence(codes, silent_token=52, max_consecutive=30)
            silence_time = time.perf_counter() - silence_start
            print(f"ğŸ”‡ [DEBUG] é™éŸ³ç§»é™¤è€—æ—¶: {silence_time:.3f}s")
            if verbose:
                print(f"fix codes shape: {codes.shape}, codes type: {codes.dtype}")
                print(f"code len: {code_lens}")
                
            # GPT forward ç”Ÿæˆlatent
            print(f"ğŸ§  [DEBUG] å¼€å§‹GPT forward...")
            gpt_forward_start = time.perf_counter()
            with torch.no_grad():  # ç¡®ä¿æ²¡æœ‰æ¢¯åº¦è®¡ç®—
                with torch.autocast(text_tokens.device.type, enabled=self.dtype is not None, dtype=self.dtype):
                    latent = self.gpt(auto_conditioning, text_tokens,
                                    torch.tensor([text_tokens.shape[-1]], device=text_tokens.device), codes,
                                    code_lens*self.gpt.mel_length_compression,
                                    cond_mel_lengths=torch.tensor([auto_conditioning.shape[-1]], device=text_tokens.device),
                                    return_latent=True, clip_inputs=False)
            gpt_forward_time = time.perf_counter() - gpt_forward_start
            cumulative_gpt_forward_time += gpt_forward_time
            print(f"âœ… [DEBUG] GPT forwardå®Œæˆï¼Œè€—æ—¶: {gpt_forward_time:.3f}s")
            
            # ğŸ” æ£€æŸ¥GPT forwardåçš„å†…å­˜
            if "cuda" in str(self.device):
                try:
                    gpu_memory_after_gpt_forward = torch.cuda.memory_allocated() / 1024**3  # GB
                    print(f"ğŸ” [DEBUG] GPT forwardåGPUå†…å­˜: {gpu_memory_after_gpt_forward:.2f} GB")
                except:
                    pass

            # BigVGAN ç”Ÿæˆwav
            print(f"ğŸµ [DEBUG] å¼€å§‹BigVGANç”Ÿæˆ...")
            bigvgan_start = time.perf_counter()
            with torch.no_grad():  # ç¡®ä¿æ²¡æœ‰æ¢¯åº¦è®¡ç®—
                wav, _ = self.bigvgan(latent, auto_conditioning.transpose(1, 2))
            bigvgan_time = time.perf_counter() - bigvgan_start
            cumulative_bigvgan_time += bigvgan_time
            print(f"âœ… [DEBUG] BigVGANå®Œæˆï¼Œè€—æ—¶: {bigvgan_time:.3f}s")
            
            # ğŸ” æ£€æŸ¥BigVGANåçš„å†…å­˜
            if "cuda" in str(self.device):
                try:
                    gpu_memory_after_bigvgan = torch.cuda.memory_allocated() / 1024**3  # GB
                    print(f"ğŸ” [DEBUG] BigVGANåGPUå†…å­˜: {gpu_memory_after_bigvgan:.2f} GB")
                except:
                    pass
            
            # ğŸ¯ ä¿®å¤éŸ³é¢‘åå¤„ç† - ç¡®ä¿Androidå…¼å®¹æ€§
            postprocess_start = time.perf_counter()
            
            # ç¡®ä¿å•å£°é“è¾“å‡º (Androidå…¼å®¹æ€§)
            if wav.dim() > 2:
                wav = wav.squeeze()
            if wav.dim() == 2:
                if wav.shape[0] == 1:
                    wav = wav.squeeze(0)  # ç§»é™¤æ‰¹æ¬¡ç»´åº¦
                elif wav.shape[1] == 1:
                    wav = wav.squeeze(1)  # ç§»é™¤å£°é“ç»´åº¦
                else:
                    wav = wav[0]  # å–ç¬¬ä¸€ä¸ªå£°é“
            
            # ç¡®ä¿wavæ˜¯ä¸€ç»´çš„
            if wav.dim() != 1:
                wav = wav.flatten()
            
            # éŸ³é¢‘å½’ä¸€åŒ–å’Œç±»å‹è½¬æ¢
            wav = torch.clamp(wav, -1.0, 1.0)  # é¦–å…ˆå½’ä¸€åŒ–åˆ°[-1,1]
            
            # è½¬æ¢ä¸ºCPUå¹¶ä¿æŒfloat32æ ¼å¼ï¼ˆæ›´å¥½çš„å…¼å®¹æ€§ï¼‰
            wav_cpu = wav.cpu().float()
            
            if verbose:
                print(f"wav shape: {wav_cpu.shape}", "min:", wav_cpu.min(), "max:", wav_cpu.max())
            
            postprocess_time = time.perf_counter() - postprocess_start
            print(f"ğŸ”„ [DEBUG] éŸ³é¢‘åå¤„ç†è€—æ—¶: {postprocess_time:.3f}s, è¾“å‡ºshape: {wav_cpu.shape}")
            
            sentence_total_time = time.perf_counter() - sentence_start_time
            
            # å‡†å¤‡æ—¶é—´ç»Ÿè®¡ä¿¡æ¯
            timing_info = {
                'sentence_total_time': sentence_total_time,
                'gpt_gen_time': gpt_gen_time,
                'gpt_forward_time': gpt_forward_time,
                'bigvgan_time': bigvgan_time,
                'token_time': token_time,
                'codes_process_time': codes_process_time,
                'silence_time': silence_time,
                'postprocess_time': postprocess_time,
                'cumulative_total_time': time.perf_counter() - start_time,
                'cumulative_gpt_gen_time': cumulative_gpt_gen_time,
                'cumulative_gpt_forward_time': cumulative_gpt_forward_time,
                'cumulative_bigvgan_time': cumulative_bigvgan_time,
            }
            
            print(f"â±ï¸ [DEBUG] å¥å­ {sentence_idx + 1} æ€»è€—æ—¶: {sentence_total_time:.3f}s")
            print(f"ğŸ“Š [DEBUG] è¯¦ç»†è€—æ—¶åˆ†å¸ƒ:")
            print(f"    - GPTç”Ÿæˆ: {gpt_gen_time:.3f}s ({gpt_gen_time/sentence_total_time*100:.1f}%)")
            print(f"    - GPTå‰å‘: {gpt_forward_time:.3f}s ({gpt_forward_time/sentence_total_time*100:.1f}%)")
            print(f"    - BigVGAN: {bigvgan_time:.3f}s ({bigvgan_time/sentence_total_time*100:.1f}%)")
            print(f"    - å…¶ä»–å¤„ç†: {(sentence_total_time-gpt_gen_time-gpt_forward_time-bigvgan_time):.3f}s")
            
            # yield å½“å‰å¥å­çš„éŸ³é¢‘ç‰‡æ®µ - è¿”å›float32æ ¼å¼çš„tensor
            yield {
                'audio_chunk': wav_cpu,  # ç°åœ¨è¿”å›torch.Tensor (float32)
                'sample_rate': sampling_rate,
                'sentence_index': sentence_idx,
                'total_sentences': total_sentences,
                'sentence_text': ' '.join(sent),  # é‡å»ºå¥å­æ–‡æœ¬ç”¨äºæ˜¾ç¤º
                'timing_info': timing_info
            }
            
            # ğŸ”¥ å¼ºåˆ¶æ¸…ç†GPUç¼“å­˜å’Œå˜é‡ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
            cache_start = time.perf_counter()
            
            # ğŸ¯ å…³é”®ï¼šåˆ›å»ºä¸´æ—¶å¼•ç”¨åˆ—è¡¨ï¼Œç¡®ä¿æ‰€æœ‰ä¸­é—´å¼ é‡éƒ½è¢«åˆ é™¤
            temp_tensors = [codes, latent, wav, wav_cpu, text_tokens]
            if 'code_lens' in locals():
                temp_tensors.append(code_lens)
            
            # æ‰¹é‡åˆ é™¤æ‰€æœ‰ä¸´æ—¶å¼ é‡
            for tensor in temp_tensors:
                if isinstance(tensor, torch.Tensor):
                    del tensor
            del temp_tensors
            
            # ğŸ§  æ¸…ç†GPTæ¨¡å‹çš„å†…éƒ¨çŠ¶æ€å’Œç¼“å­˜
            if hasattr(self.gpt, 'inference_model') and hasattr(self.gpt.inference_model, 'transformer'):
                # æ¸…ç†transformerçš„past_key_valuesç¼“å­˜
                for layer in self.gpt.inference_model.transformer.h:
                    if hasattr(layer, 'attn') and hasattr(layer.attn, 'past_key_value'):
                        try:
                            setattr(layer.attn, 'past_key_value', None)
                        except (AttributeError, TypeError):
                            # å¦‚æœè®¾ç½®å¤±è´¥ï¼Œè·³è¿‡è¯¥å±‚
                            pass
            
            # å¼ºåˆ¶æ¸…ç†GPUç¼“å­˜
            self.torch_empty_cache()
            
            # é¢å¤–çš„å¼ºåˆ¶æ¸…ç†
            if "cuda" in str(self.device):
                try:
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()  # åŒæ­¥CUDAæ“ä½œ
                    # æ¸…ç†CUDAå†…å­˜åˆ†é…å™¨çš„å†…å­˜ç¢ç‰‡
                    torch.cuda.reset_peak_memory_stats()
                except:
                    pass
            
            cache_time = time.perf_counter() - cache_start
            print(f"ğŸ§¹ [DEBUG] å¼ºåŒ–GPUç¼“å­˜æ¸…ç†è€—æ—¶: {cache_time:.3f}s")
            
            # æ£€æŸ¥æ¸…ç†åçš„GPUå†…å­˜
            if "cuda" in str(self.device):
                try:
                    gpu_memory_after = torch.cuda.memory_allocated() / 1024**3  # GB
                    print(f"ğŸ” [DEBUG] æ¸…ç†åGPUå†…å­˜: {gpu_memory_after:.2f} GB")
                except:
                    pass

        total_time = time.perf_counter() - start_time
        print(f"\nğŸ“Š [DEBUG] === æµå¼æ¨ç†å®Œæˆæ€»ç»“ ===")
        print(f">> Reference audio length: {cond_mel_frame * 256 / sampling_rate:.2f} seconds")
        print(f">> Total streaming inference time: {total_time:.2f} seconds")
        print(f">> Total gpt_gen_time: {cumulative_gpt_gen_time:.2f} seconds ({cumulative_gpt_gen_time/total_time*100:.1f}%)")
        print(f">> Total gpt_forward_time: {cumulative_gpt_forward_time:.2f} seconds ({cumulative_gpt_forward_time/total_time*100:.1f}%)")
        print(f">> Total bigvgan_time: {cumulative_bigvgan_time:.2f} seconds ({cumulative_bigvgan_time/total_time*100:.1f}%)")
        print(f">> å¹³å‡æ¯å¥è€—æ—¶: {total_time/total_sentences:.2f} seconds")
        
        # æœ€ç»ˆGPUå†…å­˜æ£€æŸ¥
        if "cuda" in str(self.device):
            try:
                gpu_memory_final = torch.cuda.memory_allocated() / 1024**3  # GB
                print(f"ğŸ” [DEBUG] æœ€ç»ˆGPUå†…å­˜: {gpu_memory_final:.2f} GB")
            except:
                pass

    def infer_stream_pcm(self, audio_prompt, text, verbose=False, max_text_tokens_per_sentence=120, **generation_kwargs):
        """
        æµå¼æ¨ç†å‡½æ•°ï¼Œé€å¥ç”ŸæˆéŸ³é¢‘ç‰‡æ®µå¹¶è¿”å›16bit PCMå­—èŠ‚æ•°æ®
        
        Args:
            audio_prompt: å‚è€ƒéŸ³é¢‘è·¯å¾„
            text: è¦åˆæˆçš„æ–‡æœ¬
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            max_text_tokens_per_sentence: æ¯å¥æœ€å¤§tokenæ•°
            **generation_kwargs: ç”Ÿæˆå‚æ•°
            
        Yields:
            dict: åŒ…å«éŸ³é¢‘ç‰‡æ®µä¿¡æ¯çš„å­—å…¸
                - audio_pcm_bytes: bytes, 16bit PCMéŸ³é¢‘æ•°æ®
                - sample_rate: int, é‡‡æ ·ç‡
                - sentence_index: int, å½“å‰å¥å­ç´¢å¼• (ä»0å¼€å§‹)
                - total_sentences: int, æ€»å¥å­æ•°
                - sentence_text: str, å½“å‰å¥å­çš„tokenæ–‡æœ¬
                - timing_info: dict, æ—¶é—´ç»Ÿè®¡ä¿¡æ¯
        """
        for chunk_info in self.infer_stream(audio_prompt, text, verbose, max_text_tokens_per_sentence, **generation_kwargs):
            # è·å–éŸ³é¢‘æ•°æ® (torch.Tensor, float32, å•å£°é“)
            audio_chunk = chunk_info['audio_chunk']
            
            # ğŸ¯ å®Œå…¨æ¨¡ä»¿inferå‡½æ•°çš„tensorå¤„ç†é€»è¾‘
            # ç¡®ä¿æ˜¯ä¸€ç»´å¼ é‡
            if audio_chunk.dim() != 1:
                audio_chunk = audio_chunk.flatten()
            
            # ç¡®ä¿å•å£°é“è¾“å‡ºï¼Œæ·»åŠ å£°é“ç»´åº¦ä»¥ç¬¦åˆå¤„ç†è¦æ±‚
            if audio_chunk.dim() == 1:
                wav = audio_chunk.unsqueeze(0)  # (length,) -> (1, length)
            else:
                wav = audio_chunk
            
            # è®¡ç®—éŸ³é¢‘é•¿åº¦ç”¨äºæ£€æŸ¥
            sampling_rate = 24000
            wav_length = wav.shape[-1] / sampling_rate
            
            # ğŸ¯ éŸ³é¢‘è´¨é‡éªŒè¯å’ŒAndroidå…¼å®¹æ€§å¤„ç† (å®Œå…¨æ¨¡ä»¿inferå‡½æ•°)
            if wav_length > 0:
                # éŸ³é¢‘è´¨é‡æ£€æŸ¥
                wav_max = wav.abs().max()
                if wav_max > 1.0:
                    if verbose:
                        print(f"âš ï¸ [DEBUG] éŸ³é¢‘è¶…å‡ºèŒƒå›´ (max={wav_max:.4f})ï¼Œè¿›è¡Œå½’ä¸€åŒ–")
                    wav = wav / wav_max
                
                # ç¡®ä¿æ²¡æœ‰NaNæˆ–Inf
                if torch.isnan(wav).any() or torch.isinf(wav).any():
                    if verbose:
                        print(f"âŒ [DEBUG] æ£€æµ‹åˆ°NaNæˆ–Infï¼Œä½¿ç”¨é›¶æ›¿æ¢")
                    wav = torch.nan_to_num(wav, nan=0.0, posinf=0.0, neginf=0.0)
                
                if verbose:
                    print(f"âœ… [DEBUG] éŸ³é¢‘è´¨é‡éªŒè¯å®Œæˆ: shape={wav.shape}, range=[{wav.min():.4f}, {wav.max():.4f}]")

            # è½¬æ¢ä¸ºint16æ ¼å¼ (å®Œå…¨æ¨¡ä»¿inferå‡½æ•°)
            wav_int16 = (wav * 32767).clamp(-32767, 32767).type(torch.int16)
            
            # è½¬æ¢ä¸ºå­—èŠ‚æ•°æ®
            pcm_bytes = wav_int16.numpy().tobytes()
            
            # ä¿®æ”¹è¿”å›çš„å­—å…¸ï¼Œæ›¿æ¢ audio_chunk ä¸º PCM å­—èŠ‚æ•°æ®
            yield {
                'audio_pcm_bytes': pcm_bytes,
                'sample_rate': chunk_info['sample_rate'],
                'sentence_index': chunk_info['sentence_index'],
                'total_sentences': chunk_info['total_sentences'],
                'sentence_text': chunk_info['sentence_text'],
                'timing_info': chunk_info['timing_info']
            }

    # åŸå§‹æ¨ç†æ¨¡å¼ - ç°åœ¨è°ƒç”¨ infer_stream å¹¶æ‹¼æ¥ç»“æœ
    def infer(self, audio_prompt, text, output_path, verbose=False, max_text_tokens_per_sentence=120, **generation_kwargs):
        """
        ä¼ ç»Ÿçš„æ¨ç†å‡½æ•°ï¼Œç°åœ¨å†…éƒ¨è°ƒç”¨ infer_stream å¹¶æ‹¼æ¥æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
        
        Args:
            audio_prompt: å‚è€ƒéŸ³é¢‘è·¯å¾„
            text: è¦åˆæˆçš„æ–‡æœ¬
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è¿”å›Gradioæ ¼å¼
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            max_text_tokens_per_sentence: æ¯å¥æœ€å¤§tokenæ•°
            **generation_kwargs: ç”Ÿæˆå‚æ•°
            
        Returns:
            str: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæŒ‡å®šäº†output_pathï¼‰
            tuple: (sample_rate, wav_data) Gradioæ ¼å¼ï¼ˆå¦‚æœoutput_pathä¸ºNoneï¼‰
        """
        start_time = time.perf_counter()
        
        # æ”¶é›†æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
        wav_chunks = []
        total_sentences = 0
        final_timing_info = None
        
        # ä½¿ç”¨æµå¼æ¨ç†æ”¶é›†æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
        for chunk_info in self.infer_stream(audio_prompt, text, verbose, max_text_tokens_per_sentence, **generation_kwargs):
            # ç°åœ¨audio_chunkå·²ç»æ˜¯float32çš„torch.Tensorï¼Œç›´æ¥æ·»åŠ 
            wav_chunks.append(chunk_info['audio_chunk'])
            total_sentences = chunk_info['total_sentences']
            final_timing_info = chunk_info['timing_info']
            
            # æ›´æ–°è¿›åº¦æ¡
            progress = (chunk_info['sentence_index'] + 1) / total_sentences
            self._set_gr_progress(0.1 + 0.8 * progress, f"Processing sentence {chunk_info['sentence_index'] + 1}/{total_sentences}")
            
            if verbose:
                print(f">> Collected chunk {chunk_info['sentence_index'] + 1}/{total_sentences}, "
                      f"shape: {chunk_info['audio_chunk'].shape}")

        self._set_gr_progress(0.9, "Concatenating audio...")
        
        # ğŸ¯ ä¿®å¤éŸ³é¢‘æ‹¼æ¥é€»è¾‘ - ç¡®ä¿Androidå…¼å®¹æ€§
        if len(wav_chunks) > 0:
            print(f"ğŸ”§ [DEBUG] æ‹¼æ¥ {len(wav_chunks)} ä¸ªéŸ³é¢‘ç‰‡æ®µ")
            
            # ç¡®ä¿æ‰€æœ‰ç‰‡æ®µéƒ½æ˜¯ä¸€ç»´çš„
            normalized_chunks = []
            for i, chunk in enumerate(wav_chunks):
                if chunk.dim() != 1:
                    chunk = chunk.flatten()
                normalized_chunks.append(chunk)
                if verbose:
                    print(f"  - ç‰‡æ®µ {i+1}: shape={chunk.shape}, min={chunk.min():.4f}, max={chunk.max():.4f}")
            
            # æ‹¼æ¥æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
            wav = torch.cat(normalized_chunks, dim=0)  # æ²¿æ—¶é—´è½´æ‹¼æ¥
            
            # ç¡®ä¿å•å£°é“è¾“å‡ºï¼Œæ·»åŠ å£°é“ç»´åº¦ä»¥ç¬¦åˆtorchaudio.saveè¦æ±‚
            if wav.dim() == 1:
                wav = wav.unsqueeze(0)  # (length,) -> (1, length)
            
            print(f"ğŸ¯ [DEBUG] æ‹¼æ¥å®Œæˆ: shape={wav.shape}, dtype={wav.dtype}")
        else:
            # å¤„ç†ç©ºç»“æœçš„æƒ…å†µ
            sampling_rate = 24000
            wav = torch.zeros((1, 0), dtype=torch.float32)
            print(f"âš ï¸ [DEBUG] æ²¡æœ‰éŸ³é¢‘ç‰‡æ®µï¼Œåˆ›å»ºç©ºéŸ³é¢‘")
            
        end_time = time.perf_counter()
        sampling_rate = 24000
        wav_length = wav.shape[-1] / sampling_rate
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        if final_timing_info:
            print(f">> Total inference time: {end_time - start_time:.2f} seconds")
            print(f">> Generated audio length: {wav_length:.2f} seconds")
            print(f">> RTF: {(end_time - start_time) / wav_length:.4f}" if wav_length > 0 else ">> RTF: N/A (no audio generated)")

        # ğŸ¯ éŸ³é¢‘è´¨é‡éªŒè¯å’ŒAndroidå…¼å®¹æ€§å¤„ç†
        if wav_length > 0:
            # éŸ³é¢‘è´¨é‡æ£€æŸ¥
            wav_max = wav.abs().max()
            if wav_max > 1.0:
                print(f"âš ï¸ [DEBUG] éŸ³é¢‘è¶…å‡ºèŒƒå›´ (max={wav_max:.4f})ï¼Œè¿›è¡Œå½’ä¸€åŒ–")
                wav = wav / wav_max
            
            # ç¡®ä¿æ²¡æœ‰NaNæˆ–Inf
            if torch.isnan(wav).any() or torch.isinf(wav).any():
                print(f"âŒ [DEBUG] æ£€æµ‹åˆ°NaNæˆ–Infï¼Œä½¿ç”¨é›¶æ›¿æ¢")
                wav = torch.nan_to_num(wav, nan=0.0, posinf=0.0, neginf=0.0)
            
            print(f"âœ… [DEBUG] éŸ³é¢‘è´¨é‡éªŒè¯å®Œæˆ: shape={wav.shape}, range=[{wav.min():.4f}, {wav.max():.4f}]")

        # ä¿å­˜æˆ–è¿”å›éŸ³é¢‘
        if output_path:
            # ä¿å­˜WAVæ–‡ä»¶ - ä½¿ç”¨Androidå…¼å®¹çš„æ ¼å¼
            if os.path.isfile(output_path):
                os.remove(output_path)
                print(">> remove old wav file:", output_path)
            if os.path.dirname(output_path) != "":
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # è½¬æ¢ä¸ºint16æ ¼å¼å¹¶ä¿å­˜ (Androidé€šç”¨æ ¼å¼)
            wav_int16 = (wav * 32767).clamp(-32767, 32767).type(torch.int16)
            
            # ä¿å­˜æ—¶æ˜ç¡®æŒ‡å®šç¼–ç æ ¼å¼ï¼Œç¡®ä¿Androidå…¼å®¹æ€§
            torchaudio.save(
                output_path, 
                wav_int16, 
                sampling_rate,
                encoding="PCM_S",  # 16-bit PCM
                bits_per_sample=16
            )
            print(f">> wav file saved to: {output_path} (format: 16-bit PCM, {sampling_rate}Hz, mono)")
            return output_path
        else:
            # è¿”å›ä»¥ç¬¦åˆGradioçš„æ ¼å¼è¦æ±‚ (int16 array)
            wav_int16 = (wav * 32767).clamp(-32767, 32767).type(torch.int16)
            wav_data = wav_int16.numpy()
            if wav_data.ndim == 2:
                wav_data = wav_data.T  # GradioæœŸæœ› (time, channels) æ ¼å¼
            return (sampling_rate, wav_data)

    def infer_opus(self, audio_prompt, text, verbose=False, max_text_tokens_per_sentence=120, 
                   opus_bitrate=32000, opus_complexity=10, **generation_kwargs):
        """
        æµå¼æ¨ç†å‡½æ•°ï¼Œé€å¥ç”ŸæˆéŸ³é¢‘ç‰‡æ®µå¹¶è¿”å› OGG å®¹å™¨ä¸­çš„ Opus ç¼–ç éŸ³é¢‘æ•°æ®æµ
        
        Args:
            audio_prompt: å‚è€ƒéŸ³é¢‘è·¯å¾„
            text: è¦åˆæˆçš„æ–‡æœ¬
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            max_text_tokens_per_sentence: æ¯å¥æœ€å¤§tokenæ•°
            opus_bitrate: Opusç¼–ç æ¯”ç‰¹ç‡ (é»˜è®¤32kbpsï¼Œå¯é€‰: 8000-512000)
            opus_complexity: Opusç¼–ç å¤æ‚åº¦ (0-10ï¼Œè¶Šé«˜è´¨é‡è¶Šå¥½ä½†ç¼–ç è¶Šæ…¢)
            **generation_kwargs: ç”Ÿæˆå‚æ•°
            
        Yields:
            bytes: OGGå®¹å™¨æ ¼å¼çš„éŸ³é¢‘æ•°æ®å— (å†…å«Opusç¼–ç éŸ³é¢‘ï¼ŒExoPlayerå®Œç¾æ”¯æŒ)
        """
        import threading
        import queue
        
        print(">> Starting OGG (Opus) streaming inference...")
        
        # æ£€æŸ¥ ffmpeg æ˜¯å¦å¯ç”¨
        try:
            subprocess.run(['ffmpeg', '-version'], capture_output=True, check=True)
        except (subprocess.CalledProcessError, FileNotFoundError):
            raise RuntimeError("FFmpeg æœªæ‰¾åˆ°ã€‚è¯·å®‰è£… FFmpeg ä»¥æ”¯æŒ Opus ç¼–ç ã€‚")
        
        # éªŒè¯ Opus å‚æ•°
        if not (8000 <= opus_bitrate <= 512000):
            raise ValueError(f"Opus bitrate must be between 8000 and 512000, got {opus_bitrate}")
        if not (0 <= opus_complexity <= 10):
            raise ValueError(f"Opus complexity must be between 0 and 10, got {opus_complexity}")
        
        if verbose:
            print(f"ğŸµ [DEBUG] Opus é…ç½®: bitrate={opus_bitrate}bps, complexity={opus_complexity}")
        
        # ğŸµ åœ¨å¾ªç¯å¤–åˆå§‹åŒ– FFmpeg è¿›ç¨‹
        opus_sample_rate = 48000  # Opus æ ‡å‡†é‡‡æ ·ç‡
        original_sample_rate = 24000  # TTS è¾“å‡ºé‡‡æ ·ç‡
        
        ffmpeg_cmd = [
            'ffmpeg',
            '-flags', 'low_delay',
            '-f', 'f32le',  # è¾“å…¥æ ¼å¼ï¼š32-bit float little-endian
            '-ar', str(original_sample_rate),  # è¾“å…¥é‡‡æ ·ç‡
            '-ac', '1',  # å•å£°é“
            '-i', 'pipe:0',  # ä» stdin è¯»å–
            '-c:a', 'libopus',  # ä½¿ç”¨ Opus ç¼–ç å™¨
            '-b:a', str(opus_bitrate),  # è®¾ç½®æ¯”ç‰¹ç‡
            '-compression_level', str(opus_complexity),  # è®¾ç½®å¤æ‚åº¦
            '-frame_duration', '100',  # 100mså¸§æŒç»­æ—¶é—´ï¼Œé€‚åˆå®æ—¶ä¼ è¾“
            '-application', 'lowdelay',  # ä¼˜å…ˆä¿è¯éŸ³é¢‘è´¨é‡å’Œå®Œæ•´æ€§
            '-ar', str(opus_sample_rate),  # è¾“å‡ºé‡‡æ ·ç‡
            '-f', 'ogg',  # ä½¿ç”¨OGGå®¹å™¨
            '-flush_packets', '1',  # å¼ºåˆ¶åˆ·æ–°åŒ…
            '-y',  # è¦†ç›–è¾“å‡º
            'pipe:1'  # è¾“å‡ºåˆ° stdout
        ]
        
        print(f"ğŸ”§ [DEBUG] å¯åŠ¨ FFmpeg: {' '.join(ffmpeg_cmd)}")
            
        
        # å¯åŠ¨ FFmpeg è¿›ç¨‹
        try:
            ffmpeg_process = subprocess.Popen(
                ffmpeg_cmd,
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                bufsize=0  # æ— ç¼“å†²
            )
        except Exception as e:
            print(f"Failed to start FFmpeg: {e}")
            raise RuntimeError(f"Failed to start FFmpeg: {e}")
        
        # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦ç«‹å³é€€å‡º
        import time
        time.sleep(0.1)
        if ffmpeg_process.poll() is not None:
            raise RuntimeError(f"FFmpeg process exited immediately with code: {ffmpeg_process.returncode}")
        
        # åˆ›å»ºè¾“å‡ºé˜Ÿåˆ—ç”¨äºå¼‚æ­¥è¯»å–
        output_queue = queue.Queue()
        error_queue = queue.Queue()
        
        def read_output():
            """å¼‚æ­¥è¯»å– FFmpeg è¾“å‡º"""
            try:
                if ffmpeg_process.stdout is not None:
                    while True:
                        # è¯»å–å›ºå®šå¤§å°çš„æ•°æ®å—
                        data = ffmpeg_process.stdout.read(4096)
                        if not data:
                            break
                        output_queue.put(data)
            except Exception as e:
                error_queue.put(f"è¯»å–è¾“å‡ºå¤±è´¥: {e}")
            finally:
                output_queue.put(None)  # ç»“æŸæ ‡è®°
        
        def read_error():
            """å¼‚æ­¥è¯»å– FFmpeg é”™è¯¯è¾“å‡º"""
            try:
                if ffmpeg_process.stderr is not None:
                    while True:
                        error_line = ffmpeg_process.stderr.readline()
                        if not error_line:
                            break
                        error_msg = error_line.decode('utf-8', errors='ignore').strip()
                        if error_msg:
                            print(f"ğŸ”´ [FFmpeg ERROR] {error_msg}")
                            error_queue.put(error_msg)
            except Exception as e:
                error_msg = f"è¯»å–é”™è¯¯è¾“å‡ºå¤±è´¥: {e}"
                print(f"ğŸ”´ [ERROR] {error_msg}")
                error_queue.put(error_msg)
        
        # å¯åŠ¨å¼‚æ­¥è¯»å–çº¿ç¨‹
        output_thread = threading.Thread(target=read_output, daemon=True)
        error_thread = threading.Thread(target=read_error, daemon=True)
        output_thread.start()
        error_thread.start()
        
        total_opus_size = 0
        chunk_count = 0
        
        # FFmpeg will generate OGG header when it receives actual audio data
        
        try:
            # ğŸµ å…ˆå‘é€ä¸€å°æ®µé™éŸ³æ¥"é¢„çƒ­"FFmpegï¼Œè¿™ä¾ç„¶æ˜¯ä¸€ä¸ªå¥½ä¹ æƒ¯
            try:
                silence_duration_ms = 20
                num_samples = int(original_sample_rate * silence_duration_ms / 1000)
                silence = torch.zeros(num_samples, dtype=torch.float32)
                if ffmpeg_process.stdin:
                    ffmpeg_process.stdin.write(silence.numpy().tobytes())
                    ffmpeg_process.stdin.flush()
                if verbose:
                    print(f"ğŸ¤ [DEBUG] Primed FFmpeg with {silence_duration_ms}ms of silence.")
            except Exception as e:
                print(f"âš ï¸ [WARNING] Failed to send priming silence to FFmpeg: {e}")

            # ğŸ”„ ä½¿ç”¨æµå¼æ¨ç†è·å–éŸ³é¢‘ç‰‡æ®µå¹¶å‘é€ç»™ FFmpeg
            for chunk_info in self.infer_stream(audio_prompt, text, verbose, max_text_tokens_per_sentence, **generation_kwargs):
                chunk_start_time = time.perf_counter()
                
                # è·å–éŸ³é¢‘æ•°æ® (torch.Tensor, float32, å•å£°é“)
                audio_chunk = chunk_info['audio_chunk']
                
                # ğŸ¯ éŸ³é¢‘é¢„å¤„ç†
                if audio_chunk.dim() != 1:
                    audio_chunk = audio_chunk.flatten()
                
                # æ£€æŸ¥éŸ³é¢‘é•¿åº¦ï¼Œé¿å…å‘é€ç©ºéŸ³é¢‘
                if audio_chunk.numel() == 0:
                    if verbose:
                        print(f"âš ï¸ [DEBUG] è·³è¿‡ç©ºéŸ³é¢‘ç‰‡æ®µ (å¥å­ {chunk_info['sentence_index'] + 1})")
                    continue
                
                # è½¬æ¢ä¸º numpy æ•°ç»„å¹¶å½’ä¸€åŒ–
                audio_np = audio_chunk.clamp(-1.0, 1.0).numpy()
                
                try:
                    # å‘é€éŸ³é¢‘æ•°æ®åˆ° FFmpeg
                    if ffmpeg_process.stdin is not None:
                        ffmpeg_process.stdin.write(audio_np.tobytes())
                        ffmpeg_process.stdin.flush()
                    
                    chunk_count += 1
                    
                    # ğŸµ è¯»å–æ‰€æœ‰å·²ç»å¯ç”¨çš„Opusè¾“å‡ºæ•°æ®
                    # åœ¨éä½å»¶è¿Ÿæ¨¡å¼ä¸‹ï¼ŒFFmpegä¼šç¼“å†²æ•°æ®ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œéé˜»å¡åœ°æ‹‰å–
                    while True:
                        try:
                            opus_data = output_queue.get_nowait()
                            if opus_data: # is not None
                                total_opus_size += len(opus_data)
                                yield opus_data
                            else: # is None, stream ended prematurely
                                break
                        except queue.Empty:
                            # é˜Ÿåˆ—ä¸ºç©ºï¼Œè¡¨ç¤ºå½“å‰æ²¡æœ‰å¯ç”¨çš„è¾“å‡ºï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªéŸ³é¢‘å—
                            break
                    
                    # æ£€æŸ¥FFmpegè¿›ç¨‹çŠ¶æ€
                    if ffmpeg_process.poll() is not None:
                        print(f"FFmpeg process exited with code: {ffmpeg_process.returncode}")
                        break
                        
                except BrokenPipeError:
                    print("âŒ [ERROR] FFmpeg è¿›ç¨‹æ„å¤–ç»ˆæ­¢")
                    break
                except Exception as e:
                    print(f"âŒ [ERROR] å‘é€æ•°æ®åˆ° FFmpeg å¤±è´¥: {e}")
                    if verbose:
                        import traceback
                        traceback.print_exc()
                    break
            
            # ğŸ”š å®Œæˆæ‰€æœ‰éŸ³é¢‘åï¼Œå…³é—­ stdin å¹¶è¯»å–å‰©ä½™è¾“å‡º
            if verbose:
                print("ğŸ”š [DEBUG] å…³é—­ FFmpeg è¾“å…¥æµ...")
                
            if ffmpeg_process.stdin is not None:
                ffmpeg_process.stdin.close()
            
            # è¯»å–å‰©ä½™çš„è¾“å‡ºæ•°æ®
            final_start = time.perf_counter()
            while True:
                try:
                    opus_data = output_queue.get(timeout=5.0)  # 5ç§’è¶…æ—¶
                    if opus_data is None:
                        break
                    elif isinstance(opus_data, bytes) and len(opus_data) > 0:
                        total_opus_size += len(opus_data)
                        if verbose:
                            print(f"ğŸ“¦ [DEBUG] æ”¶åˆ°æœ€ç»ˆ Opus æ•°æ®: {len(opus_data):,} bytes")
                        yield opus_data
                except queue.Empty:
                    # è¶…æ—¶ï¼Œå¯èƒ½æ²¡æœ‰æ›´å¤šæ•°æ®
                    if verbose:
                        print("â° [DEBUG] ç­‰å¾…æœ€ç»ˆè¾“å‡ºè¶…æ—¶")
                    break
            
            if verbose:
                final_time = time.perf_counter() - final_start
                print(f"ğŸ”š [DEBUG] æœ€ç»ˆæ•°æ®è¯»å–è€—æ—¶: {final_time:.3f}s")
        
        finally:
            # æ¸…ç†èµ„æº
            try:
                if ffmpeg_process.poll() is None:
                    ffmpeg_process.terminate()
                    ffmpeg_process.wait(timeout=5.0)
            except Exception as e:
                try:
                    ffmpeg_process.kill()
                except:
                    pass
        
        print(f">> OGG (Opus) streaming completed: {chunk_count} chunks, {total_opus_size:,} bytes")



